from census import Census #install in your environment using pip install Census
# from us import states #install in your environment using pip install us
import pandas as pd


class SocialVulnerabilityIndex:

    def __init__(self, data_catalogue, configuration):
        self.data_catalogue = data_catalogue
        self.configuration = configuration
        self.census_key = Census
        self.download_codes = {}
        self.state_fips = 0
        self.pd_census_data = pd.DataFrame()
        self.codebook = pd.DataFrame()
        self.indicator_groups = {}
        self.processed_census_data = pd.DataFrame()
        self.svi_fiat = pd.DataFrame()
        self.pd_domain_scores = pd.DataFrame()

    def set_up_census_key(self, census_key:str):
        """The Census key can be inputted in the ini file. This is a unique key that every user needs to specify to download census data"""
        self.census_key = Census(census_key)
    
    def variable_code_csv_to_pd_df(self, path:str):
        """Loading the csv in and converting to pd_dataframe to be used in the other functions.
        The Excel/CSV specifies the names of the variables that need to be downloaded from the Census API
        It contains additional information that can be called and used in the other functions.

        Parameters
        ----------
        path : specify the path where the census_vulnerability_data_codebook.csv is stored. 
        """
        self.codebook = pd.read_csv(path, sep = ";", header = 0)

    def set_up_download_codes(self):
        """From the codebook.csv we find the column with the Census code with 'E' (estimate) at the end.
        This are the codes that need to be inputted into the download_census_data function as the fields argument
        In this way the download_census_data function knows which variables to retrieve from the API.
        """
        self.download_codes = list(self.codebook["Census_code_withE"])
        self.download_codes.append('NAME')
    
    def set_up_state_code(self, state_abbreviation):
        state = [state_abbreviation] #read in the state abbreviation as specified in the ini file
        state_obj = getattr(states, state[0])
        self.state_fips = state_obj.fips
        
    def download_census_data(self):
        #download the census data
        #it is possible to also make the county, tract and blockgroup flexible so that a user could specify exactly what to download
        #But: bear in mind, with social vulneraiblity we compare areas against each other, so Ideally you would have a large enough dataset (for statistical and validity purposes)
        download_census_codes = self.census_key.acs.state_county_blockgroup(
            fields=self.download_codes, 
            state_fips=self.state_fips, 
            county_fips="*", 
            tract="*", 
            blockgroup="*"
            )
        self.pd_census_data = pd.DataFrame(download_census_codes)
        return self.pd_census_data

    def rename_census_data(self, from_name, to_name):
        #renaming the columns so that they have variable names instead of variable codes as their headers
        name_dict = dict(zip(self.codebook[from_name], self.codebook[to_name]))
        self.pd_census_data = self.pd_census_data.rename(columns = name_dict)
    
    def create_indicator_groups(self, column_to_group:str, aggregation_name:str) -> dict:
        """With this code we can group the different variables into their indicator group 
        We can also group the indicators into the domains with this code
        
        Parameters
        ----------
        column_to_group : str
            This is the column which you want to group together into an aggregate group. 
            For example grouping all the Census variables into the indicators they belong to
        aggregation_name : str
           This is the column by which you want to group. So for example: 
           this column indicates the group name for the indicators to which variables belong.

        Returns
        -------
        dict
            The dictionary matches the contents of a group to the group name (key)
        """
        for indicator_code, group_df in self.codebook.groupby(aggregation_name):
            self.indicator_groups[indicator_code] = set(group_df[column_to_group].tolist())
        return self.indicator_groups

    def processing_svi_data(self):
        """This is a function to pre-process the Census data.
        It groups the Census variables together to create the actual indicators.
        The actual indicator groups are indicated by the Indicator_full_name and Indicator_code in the codebook.
        Most of the indicators are percentages, but there are also Median values and Per Capita values.
        The preprocessing needs to be done to convert variables into indicators, for example by dividing a variable by a total * 100 to find a percentage.
        The variables with 'total' in their name, are used to do divisions for the creation of the percentages / shares.
        Therefore, this function below first finds for every indicator group if there is a variable with 'total' in the name.
        When that is not the case, the indicator is not a percentage or share but rather a Median value. 
        We can just keep that Median value and put it in the final dataframe (processed_census_data)
        When there are more than 2 columns, we first need to sum all the data (except for the total). 
        Then we compute the percentage of that sum as a share of the whole (the total). For example to find the population under 5 years and over 65 years we need to add females and males and the different age groups and then take that as a percentage of the total population.
        If there are exactly two columns, then the one needs to be divided by the other. The other is always the total.
        Together these procedures make up the processed indicators.
        """
        # create an empty dataframe to store the results
        self.processed_census_data = pd.DataFrame()
        # loop over the indicator_groups
        for key, values in self.indicator_groups.items():
            # extract columns from renamed_download_data that match 'values'
            group_vars = self.pd_census_data[values]
            # identify the columns that contain 'total' in the name. These are the columns that are used for division, to calculate percentages etc.
            total_cols = [col for col in group_vars.columns if 'total' in col.lower()]
            if len(total_cols) == 0: #if there is no total column, the indicator is a single column (e.g. Median) and we just want to take that column
                self.processed_census_data[key] = group_vars.iloc[:,0]
                # if no total columns found, skip this group
            elif len(group_vars.columns) > 2: #if there are more than 2 columns we need to first sum the columns together and then divide over the total to get a percentage
                # identify the columns that do not contain 'total' in the name
                value_cols = [col for col in group_vars.columns if col not in total_cols]
                #sum the values across the rows (axis = 1) for value columns
                sum_values = group_vars[value_cols].sum(axis=1)
                # divide the sum of values by the total column for this group
                total_col = total_cols[0]
                result = sum_values / group_vars[total_col] * 100
                # add a new column to the result dataframe with the result and the key as the column header
                self.processed_census_data[key] = result
            elif len(group_vars.columns) == 2: #if the length is exactly 2, you do not need to sum but you do need to divide over the corresponding total column
                total_col = total_cols[0]
                value_cols = [col for col in group_vars.columns if col not in total_cols]
                if 'capita' in value_cols[0]: #if 'capita' is in the column name, do not multiply by 100
                    result = group_vars[value_cols].iloc[:,0] / group_vars[total_col]
                else:
                    result = group_vars[value_cols].iloc[:,0] / group_vars[total_col] * 100
                # add a new column to the result dataframe with the result and the key as the column header
                self.processed_census_data[key] = result
        self.processed_census_data["NAME"] = self.pd_census_data["NAME"]

    #The result_of the processing is then the input for the normalization svi_data

    #General functions used in SocialVulnerabilityIndex()
    def zscore(self, dataseries):
        return (dataseries - dataseries.mean()) / dataseries.std()

    def minmax_normalize(self, dataseries):
        return (dataseries - dataseries.min()) / (dataseries.max() - dataseries.min())

    def maxmin_normalize(self, dataseries):
        return (dataseries.max() - dataseries) / (dataseries.max() - dataseries.min())
   

#FROM HERE PLEASE THOROUGHLY CHECK IF IT IS CORRECT!!

    def normalization_svi_data(self):
        # @ MARIO I now tried to implement my changes into your code. 
        #In my social_vulnerability_setup.ipynb it seems to work perfectly with codes between 0 -1
        #BUT here the scores are too high. So something goes wrong still...
                
        selected_data = self.codebook[["Indicator_code", "zscore"]]
        selected_data.drop_duplicates(subset=["Indicator_code"], inplace=True)

        vulnerability_features = selected_data["Indicator_code"].tolist()
        direction = selected_data["zscore"].tolist()

        self.svi_fiat['NAME'] = self.processed_census_data['NAME']
        svi_fiat_names = []

        for column, dir in zip(vulnerability_features, direction):
            if dir == 'normal':
                self.svi_fiat[column + '_norm'] = self.minmax_normalize(self.processed_census_data[column])
                svi_fiat_names.append(column + '_norm')
            elif dir == 'inverse':
                self.svi_fiat[column + '_norm'] = self.maxmin_normalize(self.processed_census_data[column])
                svi_fiat_names.append(column + '_norm')
            else:
                print('Direction is not provided')
    
    def domain_scores(self):
        self.pd_domain_scores = pd.DataFrame()
        self.indicator_groups = self.create_indicator_groups("Indicator_code", "Category")
        for key, values in self.indicator_groups.items():
            # Add _norm to all values to match with the normalization column names
            values_norm = [v + '_norm' for v in values]
            sum_variables = self.svi_fiat.filter(items=values_norm)
            sum_values = sum_variables.sum(axis=1) / len(values_norm)
            self.pd_domain_scores[key] = sum_values
        self.pd_domain_scores["NAME"] = self.processed_census_data["NAME"]  


#TO DO make composite scores from the domain scores and check them
    def composite_scores(self):
       self.pd_domain_scores["composite_SVI"] = self.pd_domain_scores.sum(axis=1)










# @ MARIO this is where your old code starts
    def normalization_svi_data_mario(self, df):
        #These vairables can be provided as parameters and stablished in the .ini file or through a GUI
        vulnerability_features = ["FiveYearsAgeFemBG", "FiveYearsAgeMaleBG", "TotalPopulationBG"]
        direction              = ['normal', 'normal', 'inverse']

        census_Data      = df.copy()
        svi_fiat         = pd.DataFrame()
        svi_fiat['NAME'] = census_Data['NAME']
        svi_fiat_names   = []

        for column, dir in zip(vulnerability_features, direction):
            if dir == 'normal':
                svi_fiat[column+ '_norm'] = self.minmax_normalize(df[column])
                svi_fiat_names.append(column + '_norm')
            if dir == 'inverse':
                #svi[column+'_score']       = zscore(svi[column])
                svi_fiat[column+ '_norm'] = self.maxmin_normalize(df[column]).multiply(-1)
                svi_fiat_names.append(column + '_norm')
            else:
                print('Direction is not provided')   
        return svi_fiat
    
    def scores_by_cathegory(self, df_normalized): #SEE THE EXCEL. 
        
        #@mario the categories corresponding to the variables are stored in the codebook excel in the category column!
        

        
        #These vairables can be provided as parameters and stablished in the .ini file or through a GUI. 
        #All the variable should be defined normalization_svi_data()
        dict_var        = ['FiveYearsAgeFemBG_norm','FiveYearsAgeMaleBG_norm']
        scores_vars     = [0.5,0.5]
        group_name      = 'age'

        dict_var_II     = ["TotalPopulationBG_norm"]
        scores_vars_II  = [1]
        group_name_II   = 'age_II'

        dict_var_III     = ['FiveYearsAgeFemBG_norm','FiveYearsAgeMaleBG_norm',"TotalPopulationBG_norm"]
        scores_vars_III  = False #If not provided they will provide equal weight.
        group_name_III   = 'age_III'


        def ranking(df, group_name, dict_var, scores_vars):
            if not scores_vars:
                scores_vars = [1/len(dict_var)]*len(dict_var)

            df[group_name] = 0
            for column, factor in zip(dict_var, scores_vars):
                df[group_name] = df[group_name] + df[column]*factor


        ranking(df_normalized,group_name, dict_var, scores_vars)
        ranking(df_normalized,group_name_II, dict_var_II, scores_vars_II)
        ranking(df_normalized,group_name_III, dict_var_III, scores_vars_III)

        return df_normalized







